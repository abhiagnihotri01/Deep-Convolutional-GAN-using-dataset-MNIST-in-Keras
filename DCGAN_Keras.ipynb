{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DCGAN_Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZvBbJ4IGLPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl8KTt35GNlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GETjNGKqGP2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/gdrive/My Drive/MNIST/MNIST.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/gdrive/My Drive/MNIST\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WQl03LXb7LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from time import localtime, strftime\n",
        "\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.image as mpimg \n",
        "import scipy.io\n",
        "\n",
        "\n",
        "def time():\n",
        "    return strftime('%m%d_%H%M', localtime())\n",
        "\n",
        "def make_trainable(net, val):\n",
        "    net.trainable = val\n",
        "    for l in net.layers:\n",
        "        l.trainable = val\n",
        "\n",
        "class DataLoader():\n",
        "    total_imgs = []\n",
        "    def __init__(self, dataset_name, img_res=(32, 32), mem_load=True):\n",
        "\n",
        "        self.mem_load = mem_load\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "        \n",
        "        matx = scipy.io.loadmat('/gdrive/My Drive/MNIST/Train/train_x_'+str(0)+'.mat')\n",
        "        matx = matx['train_x'];\n",
        "        self.total_imgs = matx;\n",
        "        \n",
        "        for i in range(9):\n",
        "            matx = scipy.io.loadmat('/gdrive/My Drive/MNIST/Train/train_x_'+str(i+1)+'.mat')\n",
        "            matx = matx['train_x'];\n",
        "            self.total_imgs = np.append(self.total_imgs, matx, axis=0);\n",
        "        self.total_imgs = self.total_imgs.reshape(self.total_imgs.shape[0],32,32,1)\n",
        "        self.total_imgs = self.total_imgs.astype('float32')\n",
        "        self.total_imgs = np.array(self.total_imgs) / 127.5 -1.\n",
        "        self.n_data = np.shape(self.total_imgs)[0]\n",
        "        \n",
        "     \n",
        "    def load_data(self, batch_size=1, is_testing=False):\n",
        "        imgs = [] # images to be returned\n",
        "        if self.mem_load:\n",
        "            idx = np.random.choice(range(self.n_data), size=batch_size)\n",
        "            for i in idx:\n",
        "                imgs.append(self.total_imgs[i])\n",
        "            imgs = np.array(imgs)\n",
        "        else:\n",
        "            batch_images = np.random.choice(self.path, size=batch_size)\n",
        "\n",
        "            for img_path in batch_images:\n",
        "                img = self.imread(img_path)\n",
        "                # If training => do random flip\n",
        "                # if not is_testing and np.random.random() < 0.5:\n",
        "                    # img = np.fliplr(img)\n",
        "                imgs.append(img)\n",
        "\n",
        "            imgs = np.array(imgs) / 127.5 - 1.\n",
        "        return imgs\n",
        "    \n",
        "    \n",
        "    def get_n_data(self):\n",
        "        return self.n_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bYqLJSvGSEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2DTranspose, Conv2D\n",
        "from keras.models import Model, model_from_json\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DEBUG = 0\n",
        "\n",
        "class DCGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.channels = 1\n",
        "        self.img_size = 32\n",
        "        self.latent_dim = 200\n",
        "        self.time = time()\n",
        "        self.dataset_name = 'MNIST'\n",
        "        self.learning_rate = 1e-4\n",
        "\n",
        "        optimizer = Adam(self.learning_rate, beta_1=0.5, decay=0.00005)\n",
        "\n",
        "        self.gf = 64 # filter size of generator's last layer\n",
        "        self.df = 64 # filter size of discriminator's first layer\n",
        "\n",
        "        # Configure data loader\n",
        "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
        "                                      img_res=(self.img_size, self.img_size), mem_load=True)\n",
        "        \n",
        "        \n",
        "        self.n_data = self.data_loader.get_n_data()\n",
        "\n",
        "        self.generator = self.build_generator()\n",
        "        print(\"---------------------generator summary----------------------------\")\n",
        "        self.generator.summary()\n",
        "\n",
        "        self.generator.compile(loss='mse',\n",
        "                               optimizer=optimizer,\n",
        "                               metrics=['mse'])\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(\"\\n---------------------discriminator summary----------------------------\")\n",
        "        self.discriminator.summary()\n",
        "\n",
        "        self.discriminator.compile(loss='mse',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "\n",
        "        make_trainable(self.discriminator, False)\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        fake_img = self.generator(z)\n",
        "\n",
        "        # for the combined model, we only train ganerator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        validity = self.discriminator(fake_img)\n",
        "\n",
        "        self.combined = Model([z], [validity])\n",
        "        print(\"\\n---------------------combined summary----------------------------\")\n",
        "        self.combined.summary()\n",
        "        self.combined.compile(loss=['binary_crossentropy'],\n",
        "                              optimizer=optimizer)\n",
        "        #print (\"Metrics = \"), print(self.discriminator.metrics_names)\n",
        "        #print (\"efsrfesr\")\n",
        "\n",
        "    def build_generator(self):\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "\n",
        "        def deconv2d(layer_input, filters=256, kernel_size=(5, 5), strides=(2, 2), bn_relu=True):\n",
        "            \"\"\"Layers used during upsampling\"\"\"\n",
        "            # u = UpSampling2D(size=2)(layer_input)\n",
        "            u = Conv2DTranspose(filters, kernel_size=kernel_size, strides=strides, padding='same')(layer_input)\n",
        "            if bn_relu:\n",
        "                u = BatchNormalization(momentum=0.9)(u)\n",
        "                u = Activation('relu')(u)\n",
        "            return u\n",
        "\n",
        "        generator = Dense(16 * self.gf * self.img_size // 16 * self.img_size // 16, activation=\"relu\")(noise)\n",
        "        generator = Reshape((self.img_size // 16, self.img_size // 16, self.gf * 16))(generator)\n",
        "        generator = BatchNormalization()(generator)\n",
        "        generator = Activation('relu')(generator)\n",
        "        generator = deconv2d(generator, filters=self.gf * 8)\n",
        "        generator = deconv2d(generator, filters=self.gf * 4)\n",
        "        generator = deconv2d(generator, filters=self.gf * 2)\n",
        "        generator = deconv2d(generator, filters=self.gf    )\n",
        "        generator = deconv2d(generator, filters=self.channels, kernel_size=(3,3), strides=(1,1), bn_relu=False)\n",
        "\n",
        "        gen_img = Activation('tanh')(generator)\n",
        "\n",
        "        return Model(noise, gen_img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        def d_block(layer_input, filters, strides=1, bn=True):\n",
        "            \"\"\"Discriminator layer\"\"\"\n",
        "            d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(layer_input)\n",
        "            if bn:\n",
        "                d = BatchNormalization(momentum=0.9)(d)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "\n",
        "            return d\n",
        "\n",
        "        # Input img = generated image\n",
        "        d0 = Input(shape=(self.img_size, self.img_size, self.channels))\n",
        "\n",
        "        d = d_block(d0, self.df, strides=2, bn=False)\n",
        "        d = d_block(d, self.df*2, strides=2)\n",
        "        d = d_block(d, self.df*4, strides=2)\n",
        "        d = d_block(d, self.df*8, strides=2)\n",
        "\n",
        "        d = Flatten()(d)\n",
        "        validity = Dense(1, activation='sigmoid')(d)\n",
        "\n",
        "        return Model(d0, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size, sample_interval):\n",
        "        def named_logs(model, logs):\n",
        "            result = {}\n",
        "            for l in zip(model.metrics_names, logs):\n",
        "                result[l[0]] = l[1]\n",
        "            return result\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        max_iter = int(self.n_data/batch_size)\n",
        "        os.makedirs('./logs/%s' % self.time, exist_ok=True)\n",
        "        tensorboard = TensorBoard('./logs/%s' % self.time)\n",
        "        tensorboard.set_model(self.generator)\n",
        "\n",
        "        os.makedirs('models/%s' % self.time, exist_ok=True)\n",
        "        with open('models/%s/%s_architecture.json' % (self.time, 'generator'), 'w') as f:\n",
        "            f.write(self.generator.to_json())\n",
        "        print(\"\\nbatch size : %d | num_data : %d | max iteration : %d | time : %s \\n\" % (batch_size, self.n_data, max_iter, self.time))\n",
        "        for epoch in range(1, epochs+1):\n",
        "            for iter in range(max_iter):\n",
        "                # ------------------\n",
        "                #  Train Generator\n",
        "                # ------------------\n",
        "                ref_imgs = self.data_loader.load_data(batch_size)\n",
        "\n",
        "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "                gen_imgs = self.generator.predict(noise)\n",
        "                make_trainable(self.discriminator, True)\n",
        "                d_loss_real = self.discriminator.train_on_batch(ref_imgs, valid*0.9)  # label smoothing\n",
        "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "                make_trainable(self.discriminator, False)\n",
        "\n",
        "                logs = self.combined.train_on_batch([noise], [valid])\n",
        "                tensorboard.on_epoch_end(iter, named_logs(self.combined, [logs]))\n",
        "\n",
        "                if iter % (sample_interval // 10) == 0:\n",
        "                    elapsed_time = datetime.datetime.now() - start_time\n",
        "                    #print(\"epoch:%d | iter : %d / %d | time : %10s | g_loss : %15s | d_loss : %s \" %\n",
        "                    #      (epoch, iter, max_iter, elapsed_time, logs, d_loss))\n",
        "                    print(\"epoch:%d | iter : %d / %d | time : %10s | g_loss : %15s | d_loss_real : %s | d_loss_fake : %s \" %\n",
        "                          (epoch, iter, max_iter, elapsed_time, logs, d_loss_real, d_loss_fake))\n",
        "                    \n",
        "                if (iter+1) % sample_interval == 0:\n",
        "                    self.sample_images(epoch, iter+1)\n",
        "\n",
        "            # save weights after every epoch\n",
        "            self.generator.save_weights('models/%s/%s_epoch%d_weights.h5' % (self.time, 'generator', epoch))\n",
        "            #self.discriminator.save_weights('models/%s/%s_epoch%d_weights.h5' % (self.time, 'discriminator', epoch))\n",
        "            self.discriminator.save('models/%s/%s_epoch%d_weights.hdf5' % (self.time, 'discriminator_model', epoch))\n",
        "            \n",
        "    def sample_images(self, epoch, iter):\n",
        "        os.makedirs('samples/%s' % self.time, exist_ok=True)\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r*c, self.latent_dim))\n",
        "        gen_img = self.generator.predict(noise)\n",
        "        \n",
        "        gen_img = gen_img.reshape(gen_img.shape[0],32,32)\n",
        "        # Rescale images 0 - 1\n",
        "        gen_img = 0.5 * gen_img + 0.5\n",
        "\n",
        "        # Save generated images and the high resolution originals\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        for row in range(r):\n",
        "            for col in range(c):\n",
        "                #plt.imshow(gen_img[5*row+col])\n",
        "                #plt.show()\n",
        "                axs[row, col].imshow(gen_img[5*row+col])\n",
        "                axs[row, col].axis('off')\n",
        "        plt.show()\n",
        "        #fig.savefig(\"samples/%s/e%d-i%d.png\" % (self.time, epoch, iter), bbox_inches='tight', dpi=100)\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    gan = DCGAN()\n",
        "    \n",
        "    gan.train(epochs=50, batch_size=64, sample_interval=200)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}